# Plik konfiguracyjny dla narzędzia Czytacz
# Skopiuj ten plik do .env i uzupełnij swoimi kluczami API.
#
# ~/.local/share/czytacz/.env

# Kolejność używania modeli LLM. Skrypt będzie próbował użyć modeli w tej kolejności.
# Dostępne opcje: "gemini", "openai", "deepseek", "ollama"
LLM_FALLBACK_ORDER="gemini,openai,deepseek,ollama"

# --- Konfiguracja API ---

# Google Gemini
# Klucz API można uzyskać z Google AI Studio: https://aistudio.google.com/app/apikey
GEMINI_API_KEY="Twoj_klucz_api_gemini"

# OpenAI
# Klucz API można uzyskać z panelu OpenAI: https://platform.openai.com/api-keys
OPENAI_API_KEY="Twoj_klucz_api_openai"
OPENAI_MODEL="gpt-4o" # Możesz zmienić na np. "gpt-3.5-turbo"

# DeepSeek
# Klucz API można uzyskać z panelu DeepSeek: https://platform.deepseek.com/
DEEPSEEK_API_KEY="Twoj_klucz_api_deepseek"
# Dostępne modele: "deepseek-chat", "deepseek-coder"
DEEPSEEK_MODEL="deepseek-chat"

# Ollama (dla modeli uruchamianych lokalnie)
# Podaj pełny adres URL do serwera Ollama
OLLAMA_BASE_URL="http://localhost:11434"
# Nazwa modelu, który ma być użyty (np. "llama3", "mistral")
OLLAMA_MODEL="llama3"
